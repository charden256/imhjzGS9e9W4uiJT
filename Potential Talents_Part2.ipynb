{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5ef2fb-6136-42b3-978d-7a0ab7563b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8746d17-2911-4487-ac22-775795c5d5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                          job_title  \\\n",
      "0      1  2019 C.T. Bauer College of Business Graduate (...   \n",
      "1      2  Native English Teacher at EPIK (English Progra...   \n",
      "2      3              Aspiring Human Resources Professional   \n",
      "3      4             People Development Coordinator at Ryan   \n",
      "4      5    Advisory Board Member at Celal Bayar University   \n",
      "..   ...                                                ...   \n",
      "99   100  Aspiring Human Resources Manager | Graduating ...   \n",
      "100  101              Human Resources Generalist at Loparex   \n",
      "101  102   Business Intelligence and Analytics at Travelers   \n",
      "102  103                     Always set them up for Success   \n",
      "103  104   Director Of Administration at Excellence Logging   \n",
      "\n",
      "                                location connection  fit  \n",
      "0                         Houston, Texas         85  NaN  \n",
      "1                                 Kanada      500+   NaN  \n",
      "2    Raleigh-Durham, North Carolina Area         44  NaN  \n",
      "3                          Denton, Texas      500+   NaN  \n",
      "4                         İzmir, Türkiye      500+   NaN  \n",
      "..                                   ...        ...  ...  \n",
      "99              Cape Girardeau, Missouri        103  NaN  \n",
      "100  Raleigh-Durham, North Carolina Area      500+   NaN  \n",
      "101           Greater New York City Area         49  NaN  \n",
      "102             Greater Los Angeles Area      500+   NaN  \n",
      "103                          Katy, Texas      500+   NaN  \n",
      "\n",
      "[104 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#load csv and print out the dataframe\n",
    "df = pd.read_csv('potential-talents - Aspiring human resources - seeking human resources.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924870c4-7f4a-4e54-8091-321ac08abf20",
   "metadata": {},
   "source": [
    "## Getting started with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1609a729-9fa8-4e55-8ff9-7a8274b64713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download, login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58612fcc-8520-489e-adcd-8e42dbe4a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_KEY = os.environ.get(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db32260-8cc4-4793-ac57-0634bd5d0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_KEY = \"Omitted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1639615b-3656-4d7c-992d-89ffa7b6e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(HUGGING_FACE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b5a8db5-015c-412b-9862-caf3253590c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "filenames = [\n",
    "        \"pytorch_model.bin\", \"added_tokens.json\", \"config.json\", \"generation_config.json\", \n",
    "        \"special_tokens_map.json\", \"spiece.model\", \"tokenizer_config.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05d6390f-7f9b-4fbf-9197-c4c0884fd0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: The first thing you need to know is that there are two types of language models:\n",
      "\n",
      "  • Natural language models: These are models that are trained on the data that is generated by natural language.\n",
      "  • Generative models: These are models that are trained on the data that is generated by the language model.\n",
      "\n",
      "The first thing you need to know is that there are two types of language models:\n",
      "\n",
      "  • Natural language models: These are models that are trained on the data that is generated by the language model.\n",
      "  • Generative models: These are models that are trained on the data that is generated by the language model.\n",
      "\n",
      "The second thing you need to know is that there are two types of language models:\n",
      "\n",
      "  • Natural language models: These are models that are trained on the data that is generated by the language model.\n",
      "  • Generative models: These are models that are trained on the data that is generated by the language model.\n",
      "\n",
      "The first thing you need to know is that there are two types of language models:\n",
      "\n",
      "  • Natural language models: These are models that are trained on the data that is generated by the language model.\n",
      "  • Generative models: These are models that are trained on the data that is generated by the language model.\n",
      "\n",
      "The second thing you need to know is that there are two types of language models:\n",
      "\n",
      "  • Natural language models: These are models that are trained on the data that is generated by the language model.\n",
      "  • Generative models: These are models that are trained on the data that is generated by the language model.\n",
      "\n",
      "The first thing you need to know is that there are two types of language models:\n",
      "\n",
      "  • Natural language models: These are models that are trained on the data that is generated by the language model.\n",
      "  • Generative models: These are models that are trained on the data that is generated by the language model.\n",
      "\n",
      "The second thing you need to know is that there are two types of language models:\n",
      "\n",
      "  • Natural language models: These are models that are trained on the data that is generated by the language model.\n",
      "  • Generative models: These are models that are trained on the data that is generated by the language model.\n",
      "\n",
      "The first thing you need to know is that there are two types of language models:\n",
      "\n",
      "  • Natural language models: These are models that are trained on the data that is generated by the\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    #device_map=\"cuda:0\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "#text = tokenizer.apply_chat_template(\n",
    "#    messages,\n",
    "#    tokenize=False,\n",
    "#    add_generation_prompt=True,\n",
    "#    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    "#)\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=500#32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c0d009f-f4e4-48f0-85a3-f106e5f72b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content:  What is the purpose of large language models? What is the difference between large language models and small language models? What is the role of the model in the system? What is the main purpose of the system? What are the benefits of using large language models? What are the challenges in developing large language models? What are the potential applications of large language models? What is the future of large language models? What are the ethical issues in developing large language models? What are the security issues in developing large language models? What is the role of the model in the system? What is the main purpose of the system? What are the benefits of using large language models? What are the challenges in developing large language models? What are the potential applications of large language models? What is the future of large language models? What are the ethical issues in developing large language models? What are the security issues in developing large language models? What is the role of the model in the system? What is the main purpose of the system? What are the benefits of using large language models? What are the challenges in developing large language models? What are the potential applications of large language models? What is the future of large language models? What are the ethical issues in developing large language models? What are the security issues in developing large language models? What is the role of the model in the system? What is the main purpose of the system? What are the benefits of using large language models? What are the challenges in developing large language models? What are the potential applications of large language models? What is the future of large language models? What are the ethical issues in developing large language models? What are the security issues in developing large language models? What is the role of the model in the system? What is the main purpose of the system? What are the benefits of using large language models? What are the challenges in developing large language models? What are the potential applications of large language models? What is the future of large language models? What are the ethical issues in developing large language models? What are the security issues in developing large language models? What is the role of the model in the system? What is the main purpose of the system? What are the benefits of using large language models? What are the challenges in developing large language models? What are the potential applications of large language models? What is the future of large language models? What are the ethical issues in developing large language models? What are the\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    #device_map=\"cuda:0\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "#text = tokenizer.apply_chat_template(\n",
    "#    messages,\n",
    "#    tokenize=False,\n",
    "#    add_generation_prompt=True,\n",
    "#    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    "#)\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=500#32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "622b85e2-9a9b-49e3-bc4c-b464c3fa3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install triton --upgrade --no-cache-dir\n",
    "#!pip install -U triton-windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22109c45-4d36-4ebd-81f6-69c0ae87be3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.6.0+cu126)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68591c05-dc97-493d-a591-fc0a5ab56cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a22a007-3533-4741-aab8-db0e69c56cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall torch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08448e71-a863-4ce5-bdd8-793205f90fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128 #DONT run makes a long list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "518b2b34-a61e-4174-853d-953ff74b29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#print(torch.cuda.is_available())\n",
    "#print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a57afe0-177c-490c-9b1b-8d1013760426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61037b61-9e7a-4885-9dd9-91ccefe4793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import GPT2Tokenizer, GPT2Model\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#model = GPT2Model.from_pretrained('gpt2')\n",
    "#text = \"Replace me by any text you'd like.\"\n",
    "#encoded_input = tokenizer(text, return_tensors='pt')\n",
    "#output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acee9f76-a856-4deb-9e09-b927ca653e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but let's look at how the language model itself works.\\n\\nWhat is a Language Model?\\n\\nOne of the great things about the language model is that it allows developers to get to know the language easily and quickly. Once you get used to it, it becomes really easy to understand.\\n\\nFor example\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')#, device_map='auto')\n",
    "#set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_new_tokens=64, num_return_sequences=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa302163-1920-48cb-9ab4-0a4d7f0e148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: Okay, let's tackle this problem. The user wants me to rank job titles based on their semantic similarity to the search term \"aspiring human resources\" and assign scores between 0.0 and 1.0. First, I need to understand the search term. It's about someone who is aspiring to become a human resources professional. So, the key terms here are \"aspiring\" and \"human resources.\"\n",
      "\n",
      "Looking at the list of job titles, I need to identify which ones directly relate to \"aspiring\" and \"human resources.\" Let's go through each title one by one.\n",
      "\n",
      "The first title is \"2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional.\" This seems very close. Both parts are about being a graduate with a notable achievement in business and having a passion for HR. The score here should be high, maybe around 1.0.\n",
      "\n",
      "Next, \"Native English Teacher at EPIK (English Program in Korea)\" – this is about teaching, not HR. So, not relevant. Then \"Aspiring Human Resources Professional\" is a direct match. The score here is 1.0.\n",
      "\n",
      "Looking at \"Aspiring Human Resources Specialist,\" this is similar but slightly less direct. The term \"aspiring\" is still there, so maybe 0.8 or 0.9.\n",
      "\n",
      "\"HR Senior Specialist\" and \"HR Senior Specialist\" again are similar. The score could be 0.9.\n",
      "\n",
      "\"Student at Humber College and Aspiring Human Resources Generalist\" – this is a student, so maybe 0.5.\n",
      "\n",
      "\"Seeking Human Resources Opportunities\" – this is a search term, so maybe 0.5.\n",
      "\n",
      "\"Student at Chapman University\" – again, student, so 0.5.\n",
      "\n",
      "\"Human Resources Coordinator at InterContinental Buckhead Atlanta\" – this is a coordinator, so 0.5.\n",
      "\n",
      "\"Human Resources Professional\" – this is a general term, maybe 0.5.\n",
      "\n",
      "\"HR Manager at Endemol Shine North America\" – manager, so 0.5.\n",
      "\n",
      "\"Human Resources Generalist at Loparex\" – generalist, so 0.5.\n",
      "\n",
      "\"Business Intelligence and Analytics at Travelers\" – analytics, not HR, so 0.3.\n",
      "\n",
      "\"Always set them up for Success\" – this is more about a goal, so maybe 0.3.\n",
      "\n",
      "\"Director of Human Resources North America, Groupe Beneteau\" – director, so 0.8.\n",
      "\n",
      "\"Retired Army National Guard Recruiter, office manager, seeking a position in Human Resources.\" – this is about a recruiter, so 0.7.\n",
      "\n",
      "\"Human Resources Professional\" – same as above, 0.5.\n",
      "\n",
      "After going through all the titles, I need to make sure I'm not missing any that are more or less similar. For example, \"Aspiring Human Resources Management student seeking an internship\" – this is a student, so 0.5.\n",
      "\n",
      "So, the highest similarity scores would be for titles that directly mention \"aspiring human resources\" and are in the HR field. The lowest would be for those that don't fit. I should list each title with its score and maybe a brief explanation.\n",
      "</think>\n",
      "content: Here is the ranking of the job titles based on semantic similarity to the search term **\"aspiring human resources\"**, with scores between 0.0 (least similar) and 1.0 (most similar):\n",
      "\n",
      "1. **2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional** (1.0)  \n",
      "2. **Aspiring Human Resources Professional** (1.0)  \n",
      "3. **Aspiring Human Resources Specialist** (0.8)  \n",
      "4. **HR Senior Specialist** (0.9)  \n",
      "5. **Student at Humber College and Aspiring Human Resources Generalist** (0.5)  \n",
      "6. **Seeking Human Resources Opportunities** (0.5)  \n",
      "7. **Human Resources Coordinator at InterContinental Buckhead Atlanta** (0.5)  \n",
      "8. **Human Resources Professional** (0.5)  \n",
      "9. **HR Manager at Endemol Shine North America** (0.5)  \n",
      "10. **Human Resources Generalist at Loparex** (0.5)  \n",
      "\n",
      "**Key Notes:**  \n",
      "- **Aspiring Human Resources** and **Aspiring Human Resources Professional** are the most directly relevant.  \n",
      "- Titles like **Aspiring Human Resources Specialist** and **HR Senior Specialist** are semantically similar but slightly less direct.  \n",
      "- **Student** and **Seeking Human Resources Opportunities** are less relevant.  \n",
      "\n",
      "Let me know if you'd like further refinements!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\")#,\n",
    "    #device_map=\"auto\"\n",
    "#)\n",
    "\n",
    "#Create a list of countries in Europe\n",
    "search_term = \"aspiring human resources\"\n",
    "# prepare the model input\n",
    "##job_titles = [\"software engineer\", \"human resources professional\", \"data scientist\"]\n",
    "job_titles = df.job_title.tolist()\n",
    "##prompt = \"I am going to give you a list of candidates as well as a search term. I want you to rank the candidates job titles based on their semantic and include the percentage of the match\" + \\\n",
    "##\"similarity to the search term. Here is the search term.:\" + search_term + \" \" + \"Here is the list of job titles.:\" +str(job_titles)\n",
    "prompt = \"I am going to give you a list of candidates as well as a search term. I want you to rank the candidates job titles based on their semantic similarity and include the similarity score between 1.0 to 0.0 \" + \\\n",
    "\"1.0 indicating the most similar and 0.0 indicating the least similar to the search term. Here is the search term.:\" + search_term + \" \" + \"Here is the list of job titles.:\" + str(job_titles)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False,##True,\n",
    "    enable_thinking=False##True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7eb84f0-7f6e-458e-adc8-78cdb1ffc833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': '1. Human Resources Professional - 0.0\\n2. Human Resources Generalist - 0.0\\n3. HR Manager - 0.0\\n4. HRIS and Generalist Positions - 0.0\\n5. HR Senior Specialist - 0.0\\n6. HR Coordinator - 0.0\\n7. Advisory Board Member - 0.0\\n8. People Development Coordinator - 0.0\\n9. Student at Humber College and Aspiring Human Resources Generalist - 0.0\\n10. Aspiring Human Resources Management student seeking an internship - 0.0\\n11. Human Resources Coordinator at InterContinental Buckhead Atlanta - 0.0\\n12. SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR - 0.0\\n13. Business Management Major and Aspiring Human Resources Manager - 0.0\\n14. Aspiring Human Resources Professional | Passionate about helping to create an inclusive and engaging work environment - 0.0\\n15. Human Resources Specialist at Luxottica - 0.0\\n16. Director of Human Resources North America, Groupe Beneteau -'}\n"
     ]
    }
   ],
   "source": [
    "#runs for 15min\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "#messages = [\n",
    "#    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "#    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "#]\n",
    "\n",
    "search_term = \"aspiring human resources\"\n",
    "# prepare the model input\n",
    "##job_titles = [\"software engineer\", \"human resources professional\", \"data scientist\"]\n",
    "job_titles = df.job_title.tolist()\n",
    "##prompt = \"I am going to give you a list of candidates as well as a search term. I want you to rank the candidates job titles based on their semantic and include the percentage of the match\" + \\\n",
    "##\"similarity to the search term. Here is the search term.:\" + search_term + \" \" + \"Here is the list of job titles.:\" +str(job_titles)\n",
    "prompt = \"I am going to give you a list of candidates as well as a search term. I want you to rank the candidates job titles based on their semantic similarity and include the similarity score between 1.0 to 0.0 \" + \\\n",
    "\"1.0 indicating the most similar and 0.0 indicating the least similar to the search term. Do not include the original prompt. Do not provide justifications or descriptions. Here is the search term.:\" + search_term + \" \" + \"Here is the list of job titles.:\" + str(job_titles)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3bbc2e8-6afc-431d-b829-4193df7849e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74e19d4a-759a-4c02-94c6-6e2efd1b9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55566f64-476a-4da9-9d18-0ee269755d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d06db22-eb1d-4ca9-8b2c-4e7c527afd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bitsandbytes\n",
    "#print(bitsandbytes.__version__)\n",
    "#print(bitsandbytes.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a6cd617-f3b3-4223-a7e5-dac7ef499bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall bitsandbytes -y\n",
    "#!pip install bitsandbytes --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50b55f91-c5bc-4a5e-a666-9a736f2fe8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11dd1c85-db39-4b17-ac72-7c3df338bac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl.metadata (76 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-win_amd64.whl (26.2 MB)\n",
      "   ---------------------------------------- 0.0/26.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/26.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/26.2 MB 2.6 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.6/26.2 MB 2.7 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.6/26.2 MB 2.7 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 2.4/26.2 MB 2.4 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.9/26.2 MB 2.5 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 3.1/26.2 MB 2.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 3.9/26.2 MB 2.4 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 4.5/26.2 MB 2.4 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 5.2/26.2 MB 2.5 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 6.0/26.2 MB 2.6 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 6.8/26.2 MB 2.8 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 7.6/26.2 MB 2.8 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 8.4/26.2 MB 2.9 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 9.2/26.2 MB 2.9 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 9.2/26.2 MB 2.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 9.4/26.2 MB 2.8 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 10.0/26.2 MB 2.7 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 10.5/26.2 MB 2.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 11.3/26.2 MB 2.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 11.8/26.2 MB 2.7 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 12.6/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 13.1/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.6/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.9/26.2 MB 2.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 14.4/26.2 MB 2.7 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.9/26.2 MB 2.7 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 15.5/26.2 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 15.7/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 16.3/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 16.5/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 16.8/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 17.3/26.2 MB 2.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 17.6/26.2 MB 2.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 18.1/26.2 MB 2.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 18.4/26.2 MB 2.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 18.6/26.2 MB 2.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 18.6/26.2 MB 2.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 18.9/26.2 MB 2.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 18.9/26.2 MB 2.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 18.9/26.2 MB 2.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 19.1/26.2 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 19.4/26.2 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 19.7/26.2 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 19.9/26.2 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.9/26.2 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 20.4/26.2 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 20.7/26.2 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 21.0/26.2 MB 2.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 21.2/26.2 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 21.8/26.2 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 22.0/26.2 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 22.5/26.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.8/26.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.3/26.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.6/26.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 24.1/26.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 24.9/26.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.7/26.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.2/26.2 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-win_amd64.whl (453 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-win_amd64.whl (46 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Installing collected packages: pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58d32f03-92b7-4726-918d-b5319959d2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Native English Teacher at EPIK (English Program in Korea), Aspiring Human Resources Professional, People Development Coordinator at Ryan, Advisory Board Member at Celal Bayar University, Aspiring Human Resources Specialist, Student at Humber College and Aspiring Human Resources Generalist, HR Senior Specialist, Student at Humber College and Aspiring Human Resources Generalist, Seeking Human Resources HRIS and Generalist Positions, Student at Chapman University, SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR, Human Resources Coordinator at InterContinental Buckhead Atlanta, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Native English Teacher at EPIK (English Program in Korea), Aspiring Human Resources Professional, People Development Coordinator at Ryan, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Native English Teacher at EPIK (English Program in Korea), Aspiring Human Resources Professional, People Development Coordinator at Ryan, Advisory Board Member at Celal Bayar University, Aspiring Human Resources Specialist, Student at Humber College and Aspiring Human Resources Generalist, HR Senior Specialist, Aspiring Human Resources Management student seeking an internship, Seeking Human Resources Opportunities, Aspiring Human Resources Management student seeking an internship, Seeking Human Resources Opportunities, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Native English Teacher at EPIK (English Program in Korea), Aspiring Human Resources Professional, People Development Coordinator at Ryan, Advisory Board Member at Celal Bayar University, Aspiring Human Resources Specialist, Student at Humber College and Aspiring Human Resources Generalist, HR Senior Specialist, Student at Humber College and Aspiring Human Resources Generalist, Seeking Human Resources HRIS and Generalist Positions, Student at Chapman University, SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR, Human Resources Coordinator at InterContinental Buckhead Atlanta, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Native English Teacher at EPIK (English Program in Korea), Aspiring Human Resources Professional, People Development Coordinator at Ryan, Advisory Board Member at Celal Bayar University, Aspiring Human Resources Specialist, Student at Humber College and Aspiring Human Resources Generalist, HR Senior Specialist, Student at Humber College and Aspiring Human Resources Generalist, Seeking Human Resources HRIS and Generalist Positions, Student at Chapman University, SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR, Human Resources Coordinator at InterContinental Buckhead Atlanta, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Aspiring Human Resources Professional, People Development Coordinator at Ryan, Aspiring Human Resources Specialist, HR Senior Specialist, Seeking Human Resources HRIS and Generalist Positions, Student at Chapman University, SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR, Human Resources Coordinator at InterContinental Buckhead Atlanta, Experienced Retail Manager and aspiring Human Resources Professional, Human Resources, Staffing and Recruiting Professional, Human Resources Specialist at Luxottica, Director of Human Resources North America, Groupe Beneteau, Retired Army National Guard Recruiter, office manager,  seeking a position in Human Resources., Human Resources Generalist at ScottMadden, Inc., Business Management Major and Aspiring Human Resources Manager, Aspiring Human Resources Manager, seeking internship in Human Resources., Human Resources Professional, Nortia Staffing is seeking Human Resources, Payroll & Administrative Professionals!!  (408) 709-2621, Aspiring Human Resources Professional | Passionate about helping to create an inclusive and engaging work environment, Human Resources|\nConflict Management|\nPolicies & Procedures|Talent Management|Benefits & Compensation, Human Resources Generalist at Schwan's, Liberal Arts Major. Aspiring Human Resources Analyst., Junior MES Engineer| Information Systems, Senior Human Resources Business Partner at Heil Environmental, Aspiring Human Resources Professional | An energetic and Team-Focused Leader, HR Manager at Endemol Shine North America, Human Resources professional for the world leader in GIS software, RRP Brand Portfolio Executive at JTI (Japan Tobacco International), Information Systems Specialist and Programmer with a love for data and organization., Bachelor of Science in Biology from Victoria University of Wellington, Human Resources Management Major, Director Human Resources  at EY, Undergraduate Research Assistant at Styczynski Lab, Lead Official at Western Illinois University, Seeking employment opportunities within Customer Service or Patient Care, Admissions Representative at Community medical center long beach, Seeking Human  Resources Opportunities. Open to travel and relocation., Student at Westfield State University, Student at Indiana University Kokomo - Business Management - \nRetail Manager at Delphi Hardware and Paint, Aspiring Human Resources Professional, Student, Seeking Human Resources Position, Aspiring Human Resources Manager | Graduating May 2020 | Seeking an Entry-Level Human Resources Position in St. Louis, Human Resources Generalist at Loparex, Business Intelligence and Analytics at Travelers, Always set them up for Success, Director Of Administration at Excellence Logging'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# = job_titles\u001b[39;00m\n\u001b[0;32m     29\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m job_titles\n\u001b[1;32m---> 30\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m compute_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m     34\u001b[0m         load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m         bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     36\u001b[0m         bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype,\n\u001b[0;32m     37\u001b[0m         bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1392\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1387\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   1388\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   1389\u001b[0m )\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1132\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[1;32m-> 1132\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1031\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[0;32m   1026\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m   1027\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1028\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1029\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1030\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1031\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:936\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m     _raise_if_offline_mode_is_enabled()\n\u001b[1;32m--> 936\u001b[0m     dataset_readme_path \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREPOCARD_FILENAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    943\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(dataset_readme_path))\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LocalEntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[0;32m    104\u001b[0m ):\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have -- or .. in repo_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Native English Teacher at EPIK (English Program in Korea), Aspiring Human Resources Professional, People Development Coordinator at Ryan, Advisory Board Member at Celal Bayar University, Aspiring Human Resources Specialist, Student at Humber College and Aspiring Human Resources Generalist, HR Senior Specialist, Student at Humber College and Aspiring Human Resources Generalist, Seeking Human Resources HRIS and Generalist Positions, Student at Chapman University, SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR, Human Resources Coordinator at InterContinental Buckhead Atlanta, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Native English Teacher at EPIK (English Program in Korea), Aspiring Human Resources Professional, People Development Coordinator at Ryan, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Native English Teacher at EPIK (English Program in Korea), Aspiring Human Resources Professional, People Development Coordinator at Ryan, Advisory Board Member at Celal Bayar University, Aspiring Human Resources Specialist, Student at Humber College and Aspiring Human Resources Generalist, HR Senior Specialist, Aspiring Human Resources Management student seeking an internship, Seeking Human Resources Opportunities, Aspiring Human Resources Management student seeking an internship, Seeking Human Resources Opportunities, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Native English Teacher at EPIK (English Program in Korea), Aspiring Human Resources Professional, People Development Coordinator at Ryan, Advisory Board Member at Celal Bayar University, Aspiring Human Resources Specialist, Student at Humber College and Aspiring Human Resources Generalist, HR Senior Specialist, Student at Humber College and Aspiring Human Resources Generalist, Seeking Human Resources HRIS and Generalist Positions, Student at Chapman University, SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR, Human Resources Coordinator at InterContinental Buckhead Atlanta, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Native English Teacher at EPIK (English Program in Korea), Aspiring Human Resources Professional, People Development Coordinator at Ryan, Advisory Board Member at Celal Bayar University, Aspiring Human Resources Specialist, Student at Humber College and Aspiring Human Resources Generalist, HR Senior Specialist, Student at Humber College and Aspiring Human Resources Generalist, Seeking Human Resources HRIS and Generalist Positions, Student at Chapman University, SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR, Human Resources Coordinator at InterContinental Buckhead Atlanta, 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional, Aspiring Human Resources Professional, People Development Coordinator at Ryan, Aspiring Human Resources Specialist, HR Senior Specialist, Seeking Human Resources HRIS and Generalist Positions, Student at Chapman University, SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR, Human Resources Coordinator at InterContinental Buckhead Atlanta, Experienced Retail Manager and aspiring Human Resources Professional, Human Resources, Staffing and Recruiting Professional, Human Resources Specialist at Luxottica, Director of Human Resources North America, Groupe Beneteau, Retired Army National Guard Recruiter, office manager,  seeking a position in Human Resources., Human Resources Generalist at ScottMadden, Inc., Business Management Major and Aspiring Human Resources Manager, Aspiring Human Resources Manager, seeking internship in Human Resources., Human Resources Professional, Nortia Staffing is seeking Human Resources, Payroll & Administrative Professionals!!  (408) 709-2621, Aspiring Human Resources Professional | Passionate about helping to create an inclusive and engaging work environment, Human Resources|\nConflict Management|\nPolicies & Procedures|Talent Management|Benefits & Compensation, Human Resources Generalist at Schwan's, Liberal Arts Major. Aspiring Human Resources Analyst., Junior MES Engineer| Information Systems, Senior Human Resources Business Partner at Heil Environmental, Aspiring Human Resources Professional | An energetic and Team-Focused Leader, HR Manager at Endemol Shine North America, Human Resources professional for the world leader in GIS software, RRP Brand Portfolio Executive at JTI (Japan Tobacco International), Information Systems Specialist and Programmer with a love for data and organization., Bachelor of Science in Biology from Victoria University of Wellington, Human Resources Management Major, Director Human Resources  at EY, Undergraduate Research Assistant at Styczynski Lab, Lead Official at Western Illinois University, Seeking employment opportunities within Customer Service or Patient Care, Admissions Representative at Community medical center long beach, Seeking Human  Resources Opportunities. Open to travel and relocation., Student at Westfield State University, Student at Indiana University Kokomo - Business Management - \nRetail Manager at Delphi Hardware and Paint, Aspiring Human Resources Professional, Student, Seeking Human Resources Position, Aspiring Human Resources Manager | Graduating May 2020 | Seeking an Entry-Level Human Resources Position in St. Louis, Human Resources Generalist at Loparex, Business Intelligence and Analytics at Travelers, Always set them up for Success, Director Of Administration at Excellence Logging'."
     ]
    }
   ],
   "source": [
    "#from datasets import load_dataset\n",
    "#from transformers import (\n",
    "#    AutoModelForCausalLM,\n",
    "#    AutoTokenizer,\n",
    "#    BitsAndBytesConfig,\n",
    "#    HfArgumentParser,\n",
    "#    AutoTokenizer,\n",
    "#    TrainingArguments,\n",
    "#    Trainer,\n",
    "#    GenerationConfig\n",
    "#)\n",
    "from tqdm import tqdm\n",
    "#from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "\n",
    "interpreter_login()\n",
    "\n",
    "import os\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\"\n",
    "\n",
    "# = job_titles\n",
    "dataset_name = job_titles\n",
    "dataset = load_dataset(\", \".join(dataset_name))\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "device_map = {\"\": 0}\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,100,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e41f716a-ebf9-4520-ba01-4d7c3ca6d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.4.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.55.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (2.6.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bumbl\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bumbl\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 3.4.1\n",
      "    Uninstalling sentence-transformers-3.4.1:\n",
      "      Successfully uninstalled sentence-transformers-3.4.1\n",
      "Successfully installed sentence-transformers-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1d969e1-8296-4777-9944-a4357252c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-26 12:31:47 - Use pytorch device: cuda:0\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-26 12:31:48 - Use pytorch device: cuda:0\n",
      "2025-08-26 12:31:49 - Read the gooaq training dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model max length: 8192\n",
      "Model num labels: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 12:31:50 - Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 9000\n",
      "})\n",
      "2025-08-26 12:31:50 - Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "2025-08-26 12:31:50 - Load pretrained SentenceTransformer: sentence-transformers/static-retrieval-mrl-en-v1\n",
      "2025-08-26 12:31:50 - The `margin` parameter is deprecated. Use the `absolute_margin` and/or `relative_margin` parameter instead. Setting `absolute_margin` to `0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb88e580a4e4a63a8e8f6e490402bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d517c32dce4bc4b4f4e793d20595bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative candidates mined, preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 12:31:55 - Dataset({\n",
      "    features: ['question', 'answer', 'label'],\n",
      "    num_rows: 53803\n",
      "})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric       Positive       Negative     Difference\n",
      "Count           9,000         44,803               \n",
      "Mean           0.5897         0.3965         0.1952\n",
      "Median         0.6027         0.3876         0.1733\n",
      "Std            0.1426         0.1136         0.1404\n",
      "Min            0.0027         0.1295         0.0000\n",
      "25%            0.4996         0.3110         0.0788\n",
      "50%            0.6027         0.3876         0.1733\n",
      "75%            0.6914         0.4732         0.2878\n",
      "Max            0.9605         0.8515         0.7341\n",
      "Skipped 16,546 potential negatives (1.82%) due to the absolute_margin of 0.\n",
      "Could not find enough negatives for 197 samples (0.44%). Consider adjusting the range_max and absolute_margin parameters if you'd like to find more valid negatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting range_max to 31 based on the provided parameters.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'Column' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 198\u001b[0m\n\u001b[0;32m    190\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    191\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError uploading model to the Hugging Face Hub:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mTo upload it manually, you can run \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login`, followed by loading the model using `model = CrossEncoder(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_output_dir\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand saving it using `model.push_to_hub(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m         )\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 198\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 110\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    102\u001b[0m nano_beir_evaluator \u001b[38;5;241m=\u001b[39m CrossEncoderNanoBEIREvaluator(\n\u001b[0;32m    103\u001b[0m     dataset_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsmarco\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnfcorpus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnq\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    104\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mtrain_batch_size,\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# 4b. Define a reranking evaluator by mining hard negatives given query-answer pairs\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# We include the positive answer in the list of negatives, so the evaluator can use the performance of the\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# embedding model as a baseline.\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m hard_eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mmine_hard_negatives\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the full dataset as the corpus\u001b[39;49;00m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_negatives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# How many documents to rerank\u001b[39;49;00m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_positives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn-tuple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_faiss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(hard_eval_dataset)\n\u001b[0;32m    121\u001b[0m reranking_evaluator \u001b[38;5;241m=\u001b[39m CrossEncoderRerankingEvaluator(\n\u001b[0;32m    122\u001b[0m     samples\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    123\u001b[0m         {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m     always_rerank_positives\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    133\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\util\\hard_negatives.py:320\u001b[0m, in \u001b[0;36mmine_hard_negatives\u001b[1;34m(dataset, model, anchor_column_name, positive_column_name, corpus, cross_encoder, range_min, range_max, max_score, min_score, absolute_margin, relative_margin, num_negatives, sampling_strategy, query_prompt_name, query_prompt, corpus_prompt_name, corpus_prompt, include_positives, output_format, batch_size, faiss_batch_size, use_faiss, use_multi_process, verbose, cache_folder, as_triplets, margin)\u001b[0m\n\u001b[0;32m    316\u001b[0m     corpus \u001b[38;5;241m=\u001b[39m positives\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# Deduplicate the corpus\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;66;03m# make sure all the positives are also in the corpus and de-duplicate it.\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(\u001b[43mcorpus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpositives\u001b[49m))\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# corpus_idx maps the corpus text into its position in the corpus\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# This position does not necessarily matches the original corpus, as it was de-duplicated.\u001b[39;00m\n\u001b[0;32m    324\u001b[0m corpus_idx \u001b[38;5;241m=\u001b[39m {text: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(corpus)}\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Column' and 'list'"
     ]
    }
   ],
   "source": [
    "##Fine-tuning\n",
    "#Dataset\n",
    "#search_term\n",
    "#job_titles\n",
    "\n",
    "#Loss Function\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.losses import CachedMultipleNegativesRankingLoss\n",
    "\n",
    "# Load a model to train/finetune\n",
    "model = CrossEncoder(\"xlm-roberta-base\", num_labels=1) # num_labels=1 is for rerankers\n",
    "\n",
    "# Initialize the CachedMultipleNegativesRankingLoss, which requires pairs of\n",
    "# related texts or triplets\n",
    "loss = CachedMultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Load an example training dataset that works with our loss function:\n",
    "#train_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train\")\n",
    "train_dataset = job_titles\n",
    "\n",
    "#Trainer\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.cross_encoder import (\n",
    "    CrossEncoder,\n",
    "    CrossEncoderModelCardData,\n",
    "    CrossEncoderTrainer,\n",
    "    CrossEncoderTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.cross_encoder.evaluation import (\n",
    "    CrossEncoderNanoBEIREvaluator,\n",
    "    CrossEncoderRerankingEvaluator,\n",
    ")\n",
    "from sentence_transformers.cross_encoder.losses.BinaryCrossEntropyLoss import BinaryCrossEntropyLoss\n",
    "from sentence_transformers.evaluation.SequentialEvaluator import SequentialEvaluator\n",
    "from sentence_transformers.util import mine_hard_negatives\n",
    "\n",
    "# Set the log level to INFO to get more information\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO)\n",
    "\n",
    "\n",
    "def main():\n",
    "    model_name = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "    train_batch_size = 16\n",
    "    num_epochs = 1\n",
    "    num_hard_negatives = 5  # How many hard negatives should be mined for each question-answer pair\n",
    "\n",
    "    # 1a. Load a model to finetune with 1b. (Optional) model card data\n",
    "    model = CrossEncoder(\n",
    "        model_name,\n",
    "        model_card_data=CrossEncoderModelCardData(\n",
    "            language=\"en\",\n",
    "            license=\"apache-2.0\",\n",
    "            model_name=\"ModernBERT-base trained on GooAQ\",\n",
    "        ),\n",
    "    )\n",
    "    print(\"Model max length:\", model.max_length)\n",
    "    print(\"Model num labels:\", model.num_labels)\n",
    "\n",
    "    # 2a. Load the GooAQ dataset: https://huggingface.co/datasets/sentence-transformers/gooaq\n",
    "    logging.info(\"Read the gooaq training dataset\")\n",
    "    full_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train\").select(range(10_000))\n",
    "    dataset_dict = full_dataset.train_test_split(test_size=1_000, seed=12)\n",
    "    train_dataset = dataset_dict[\"train\"]\n",
    "    eval_dataset = dataset_dict[\"test\"]\n",
    "    logging.info(train_dataset)\n",
    "    logging.info(eval_dataset)\n",
    "\n",
    "    # 2b. Modify our training dataset to include hard negatives using a very efficient embedding model\n",
    "    embedding_model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "    hard_train_dataset = mine_hard_negatives(\n",
    "        train_dataset,\n",
    "        embedding_model,\n",
    "        num_negatives=num_hard_negatives,  # How many negatives per question-answer pair\n",
    "        margin=0,  # Similarity between query and negative samples should be x lower than query-positive similarity\n",
    "        range_min=0,  # Skip the x most similar samples\n",
    "        range_max=100,  # Consider only the x most similar samples\n",
    "        sampling_strategy=\"top\",  # Sample the top negatives from the range\n",
    "        batch_size=128,  # Use a batch size of 4096 for the embedding model\n",
    "        output_format=\"labeled-pair\",  # The output format is (query, passage, label), as required by BinaryCrossEntropyLoss\n",
    "        use_faiss=False,\n",
    "    )\n",
    "    logging.info(hard_train_dataset)\n",
    "\n",
    "    # 2c. (Optionally) Save the hard training dataset to disk\n",
    "    # hard_train_dataset.save_to_disk(\"gooaq-hard-train\")\n",
    "    # Load again with:\n",
    "    # hard_train_dataset = load_from_disk(\"gooaq-hard-train\")\n",
    "\n",
    "    # 3. Define our training loss.\n",
    "    # pos_weight is recommended to be set as the ratio between positives to negatives, a.k.a. `num_hard_negatives`\n",
    "    loss = BinaryCrossEntropyLoss(model=model, pos_weight=torch.tensor(num_hard_negatives))\n",
    "\n",
    "    # 4a. Define evaluators. We use the CrossEncoderNanoBEIREvaluator, which is a light-weight evaluator for English reranking\n",
    "    nano_beir_evaluator = CrossEncoderNanoBEIREvaluator(\n",
    "        dataset_names=[\"msmarco\", \"nfcorpus\", \"nq\"],\n",
    "        batch_size=train_batch_size,\n",
    "    )\n",
    "\n",
    "    # 4b. Define a reranking evaluator by mining hard negatives given query-answer pairs\n",
    "    # We include the positive answer in the list of negatives, so the evaluator can use the performance of the\n",
    "    # embedding model as a baseline.\n",
    "    hard_eval_dataset = mine_hard_negatives(\n",
    "        eval_dataset,\n",
    "        embedding_model,\n",
    "        corpus=full_dataset[\"answer\"],  # Use the full dataset as the corpus\n",
    "        num_negatives=30,  # How many documents to rerank\n",
    "        batch_size=128,\n",
    "        include_positives=False,\n",
    "        output_format=\"n-tuple\",\n",
    "        use_faiss=False,\n",
    "    )\n",
    "    logging.info(hard_eval_dataset)\n",
    "    reranking_evaluator = CrossEncoderRerankingEvaluator(\n",
    "        samples=[\n",
    "            {\n",
    "                \"query\": sample[\"question\"],\n",
    "                \"positive\": [sample[\"answer\"]],\n",
    "                \"documents\": [sample[column_name] for column_name in hard_eval_dataset.column_names[2:]],\n",
    "            }\n",
    "            for sample in hard_eval_dataset\n",
    "        ],\n",
    "        batch_size=train_batch_size,\n",
    "        name=\"gooaq-dev\",\n",
    "        always_rerank_positives=True,\n",
    "    )\n",
    "\n",
    "    # 4c. Combine the evaluators & run the base model on them\n",
    "    evaluator = SequentialEvaluator([nano_beir_evaluator])\n",
    "    evaluator(model)\n",
    "\n",
    "    # 5. Define the training arguments\n",
    "    short_model_name = model_name if \"/\" not in model_name else model_name.split(\"/\")[-1]\n",
    "    run_name = f\"reranker-{short_model_name}-gooaq-bce\"\n",
    "    args = CrossEncoderTrainingArguments(\n",
    "        # Required parameter:\n",
    "        output_dir=f\"models/{run_name}\",\n",
    "        # Optional training parameters:\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        per_device_eval_batch_size=train_batch_size,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "        bf16=True,  # Set to True if you have a GPU that supports BF16\n",
    "        dataloader_num_workers=4,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_gooaq-dev_ndcg@10\",\n",
    "        # Optional tracking/debugging parameters:\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=4000,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=4000,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=1000,\n",
    "        logging_first_step=True,\n",
    "        run_name=run_name,  # Will be used in W&B if `wandb` is installed\n",
    "        seed=12,\n",
    "    )\n",
    "\n",
    "    # 6. Create the trainer & start training\n",
    "    trainer = CrossEncoderTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=hard_train_dataset,\n",
    "        loss=loss,\n",
    "        evaluator=evaluator,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # 7. Evaluate the final model, useful to include these in the model card\n",
    "    evaluator(model)\n",
    "\n",
    "    # 8. Save the final model\n",
    "    final_output_dir = f\"models/{run_name}/final\"\n",
    "    model.save_pretrained(final_output_dir)\n",
    "\n",
    "    # 9. (Optional) save the model to the Hugging Face Hub!\n",
    "    # It is recommended to run `huggingface-cli login` to log into your Hugging Face account first\n",
    "    try:\n",
    "        model.push_to_hub(run_name)\n",
    "    except Exception:\n",
    "        logging.error(\n",
    "            f\"Error uploading model to the Hugging Face Hub:\\n{traceback.format_exc()}To upload it manually, you can run \"\n",
    "            f\"`huggingface-cli login`, followed by loading the model using `model = CrossEncoder({final_output_dir!r})` \"\n",
    "            f\"and saving it using `model.push_to_hub('{run_name}')`.\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87146e04-67b9-438b-a28b-4cc5398f564b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
